\contentsline {table}{\numberline {2.1}{\ignorespaces Nested contrast coding to investigate the effect of intrusion in grammatical and ungrammatical agreement and reflexive constructions. The contrast dep is the main effect of dependency type (agreement or reflexive). The abbreviation intr.au means intrusion (interference effect) in agreement dependencies, ungrammatical; intr.ag stands for intrusion (interference effect) in agreement dependencies, grammatical; intr.ru refers to intrusion (interference effect) in reflexive dependencies, ungrammatical; intr.rg stands for intrusion (interference effect) in reflexive dependencies, grammatical.}}{42}{table.2.1}% 
\contentsline {table}{\numberline {2.2}{\ignorespaces Summary of the sensitivity analysis, investigating the effect of incorporating prior knowledge from: mildly uninformative priors; a meta-analysis of existing reading data on ungrammatical agreement and reflexives; and the model predictions in Engelmann, J\"ager, and Vasishth, 2019. The dependent measure in the analysis is total fixation time and the posterior estimates are back-transformed to the ms scale from log ms. The priors are shown in the ms scales.}}{48}{table.2.2}% 
\contentsline {table}{\numberline {3.1}{\ignorespaces Results of the J\"ager et al.\ (2017) meta-analysis showing mean effect estimates $\mathaccentV {bar}016{b}$ with Bayesian 95\% credible intervals in the Estimates column. The range specified by a 95\% credible interval contains the true value of the estimated parameter with 95\% certainty, given the model and the data. A positive interference effect means inhibition, a negative one facilitation. Results are compared with the predictions of cue-based retrieval as implemented in the LV05 ACT-R model, and the additional contributions of the extensions \emph {item prominence} (IP) and \emph {multi-associative cues} (MAC), which are discussed in Chapter\nobreakspace {}\ref {c02prominence}.}}{64}{table.3.1}% 
\contentsline {table}{\numberline {4.1}{\ignorespaces Possible feature combinations exhibited by correct antecedents of English reflexives, reciprocals, and Chinese \textit {ziji}.}}{89}{table.4.1}% 
\contentsline {table}{\numberline {4.2}{\ignorespaces Root-mean-square deviation between modelling results and observed data, averaged within dependency type and model (best values in bold). The superscript no dec means that the decay parameter is set to 0.}}{103}{table.4.2}% 
\contentsline {table}{\numberline {4.3}{\ignorespaces Estimated values for prominence parameter in the LV05+IP+MAC model with decay for three prominence levels.}}{103}{table.4.3}% 
\contentsline {table}{\numberline {4.4}{\ignorespaces Shown here is the terminology used in the present chapter in relation to cue-based retrieval and interference in dependency resolution.}}{117}{table.4.4}% 
\contentsline {table}{\numberline {4.5}{\ignorespaces Shown here is the terminology used in the extension of the cue-based retrieval model (continued from previous page).}}{118}{table.4.5}% 
\contentsline {table}{\numberline {4.6}{\ignorespaces List of experiments included in the simulations.}}{119}{table.4.6}% 
\contentsline {table}{\numberline {4.7}{\ignorespaces List of experiments included in the simulations (continued from previous page).}}{120}{table.4.7}% 
\contentsline {table}{\numberline {4.8}{\ignorespaces Model parameters, their default values, and the values used in the simulation of the studies in the meta-analysis.}}{121}{table.4.8}% 
\contentsline {table}{\numberline {5.1}{\ignorespaces Frequency classes used in the analyses of the Schilling Corpus (SC) and Potsdam Sentence Corpus (PSC).}}{127}{table.5.1}% 
\contentsline {table}{\numberline {5.2}{\ignorespaces Fit and parameter estimates for all simulations. The interpretation of the data are discussed in the results and discussion section.}}{128}{table.5.2}% 
\contentsline {table}{\numberline {5.3}{\ignorespaces Linear regression results for predictors retrieval and surprisal}}{147}{table.5.3}% 
\contentsline {table}{\numberline {6.1}{\ignorespaces ACT-R/EMMA parameter values.}}{150}{table.6.1}% 
\contentsline {table}{\numberline {7.1}{\ignorespaces Model comparison using K-fold cross-validation for the Gibson and Wu 2013 data. Shown are the differences in $\mathaccent "0362\relax {elpd}$, along with standard errors of the differences. In a comparison between a model A vs B, a positive $\Delta \mathaccent "0362\relax {elpd}$ favours model A.}}{179}{table.7.1}% 
\contentsline {table}{\numberline {7.2}{\ignorespaces Model comparison using k-fold cross-validation for the Vasishth et al.\ (2013) replication of the Gibson and Wu (2013) study.}}{180}{table.7.2}% 
\contentsline {table}{\numberline {7.3}{\ignorespaces Comparison of the 10 sets of hierarchical models. Shown are the differences in $\mathaccent "0362\relax {elpd}$ between (a) the standard hierarchical model and the homogeneous variance mixture model; (b) the feature percolation model and the homogeneous variance mixture model; and (c) the homogeneous vs.\ heterogeneous variance mixture model. Also shown are standard errors for each comparison. If the difference in $\mathaccent "0362\relax {elpd}$ is positive, this is evidence in favour of the second model. The pairwise model comparisons are transitive. These comparisons show that the heterogeneous variance mixture model has the best predictive performance.}}{185}{table.7.3}% 
\contentsline {table}{\numberline {8.1}{\ignorespaces A matrix showing how the models relate to each other along dimensions of the three working-memory related events---delays, forgetting (or failure to retrieve), mis-retrieval---that have been investigated in sentence comprehension research.}}{198}{table.8.1}% 
\contentsline {table}{\numberline {8.2}{\ignorespaces The number of participants in subject / object relatives (SR/OR) for which non-default parameter values were predicted, in the subject vs.\ object relative tasks, respectively; for goal activation (GA), default action time (DAT) and noise (ANS) parameters.}}{204}{table.8.2}% 
\contentsline {table}{\numberline {8.3}{\ignorespaces Discrimination ability of hierarchical clustering on the combined data for subject / object relatives. Numbers in bold show the number of correctly clustered data points. The bottom row shows the percentage accuracy.}}{206}{table.8.3}% 
