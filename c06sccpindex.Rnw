\chapter[Reanalysis and underspecification]{Reanalysis and underspecification in sentence comprehension: Modelling eye movements} \label{c04}

\section{Introduction}

<<setupsims,include=FALSE,cache=FALSE>>=
library(knitr)
library(coda)
options(replace.assign=TRUE,width=75)
opts_chunk$set(dev='postscript')
set.seed(9991)
convert2log <- function(x){
  ifelse(x>=1, log(x), ifelse(x<=-1, -log(abs(x)), 0))
}

convert2log10 <- function(x){
  ifelse(x>=1, log10(x), ifelse(x<=-1, -log10(abs(x)), 0))
}
ci <- function (x) {
  m <- mean(x, na.rm = TRUE)
  n <- length(x[!is.na(x)])
  s <- sd(x, na.rm = TRUE)
  upper <- m + qt(0.975, df = n - 1) * (s/sqrt(n))
  lower <- m + qt(0.025, df = n - 1) * (s/sqrt(n))
  return(data.frame(lower = lower, upper = upper))
}
@

The previous chapter presented a model of an explicit link between the theory of parsing and memory access by \cite{LewisVasishth2005} and the EMMA model of eye movement control by \cite{Salvucci2001}.
The model parameters were estimated in an evaluation on the Potsdam Sentence Corpus \citep{Kliegl2004} using pre-computed values for memory retrieval latency and surprisal from \cite{BostonHaleVasishth2011}. However, the corpus mainly contained relatively simple, short sentences, which may be of limited value for testing concrete examples of parsing difficulty. In the current chapter, the resulting parameter estimates will therefore be used to generate new predictions for example sentences from the literature. In doing so, the fit between the data and the model can be evaluated by comparing model predictions to empirical data in response to specific types of parsing difficulty. 
The advantage of the model presented here is that it is integrated with the fully specified parser by \cite{LewisVasishth2005}, so that it generates predictions in runtime without the need to pre-compute a retrieval metric. For reasons of simplicity, the focus of this chapter is on effects of memory retrieval and not surprisal.

In addition to Interface I, \emph{Time Out}, that was presented in the previous chapter, three further elementary interfaces are proposed. Interface II, \index{Reanalysis} \emph{Reanalysis}, is an early detection of parsing error resulting in a regression similar to ``rapid integration failure'' in \cite{ReichleWarrenMcConnell2009}.
A simulation with Interfaces I and II replicates the results of \cite{Staub2010a}, who found effects of memory and expectation in distinct locations of object- vs.\ subject-relative clauses. 
Interface III, \index{Underspecification} \emph{Underspecification}, aborts a costly attachment alternatively to signalling a time out depending on the task-relevance of the attachment relation.
A second simulation illustrates how the model predicts that underspecification results from an interaction of eye movement control with parsing and individual differences in working memory capacity.
While Interfaces I and II are interventions by the parser that interrupt the otherwise autonomous saccade programming, Interface III is rather an intervention in the other direction: In the case of underspecification, the parser is cut off by time pressure imposed by eye-movement control.
Finally, a possible Interface IV: \index{Subvocalization} \emph{Subvocalization} is proposed; this is not discussed in the book but is available in the model code and is discussed in \cite{engelmann:phd}. Interface IV serves as another alternative to Time Out; a word ready for integration could be stored in phonological memory \citep{BaddeleyHitch1974,Baddeley2003} for a very short time until there is free capacity of the sentence processor. This could be used in future work to model spill-over effects of parsing difficulty.

Table~\ref{tab:params} summarizes relevant parameter values used in the simulations in this chapter. If not stated otherwise, ACT-R and EMMA parameters were kept constant at the values estimated for model $4$ ``EMMA+r'' in the corpus study discussed in chapter~\ref{c02emma} or at default values.


\begin{table}[!htbp]
\begin{center}
\begin{threeparttable}
  \begin{tabular}{lrrrrrrr}
  %\toprule
  Simulation & LF & ANS & MAS & MP & VEF & VEE & SPT \\
  %\midrule
  \ref{sec:topics:psc} PSC EMMA+r & 0.2 & 0.15 & 1.5 & 1.4 & 0.002 & 0.4 & 0.110 \\
  \ref{sec:sim:II} Relative clauses & 0.2 & 0.15 & 1.5 & 1.4 & 0.002 & 0.4 & 0.110 \\
  \ref{sec:sim:III} Underspecification & 0.2 & 0.15 & 3.5 & NIL & 0.002 & 0.4 & 0.110 \\
  %\bottomrule
  \end{tabular}
  \begin{tablenotes}
    \item \emph{Note.} LF: latency factor, ANS: activation noise, MAS: maximum associative strength, MP: mismatch penalty, VEF: visual encoding factor, VEE: visual encoding exponent, SPT: saccade preparation time.
  \end{tablenotes}
\end{threeparttable}
\end{center}
  \caption{ACT-R/EMMA parameter values.}  \label{tab:params}
\end{table}

\section[Modelling reanalysis]{Modelling reanalysis: Memory and expectation processes in parsing}\label{sec:sim:II}
\subsection{Memory and Expectation in Relative Clauses}

\cite{Staub2010a} conducted an experiment that showed effects of both memory retrieval and expectation\index{expectation} at different positions within the same sentence. \cite{Staub2010a} studied the well-known difference between \index{subject-extracted structure} subject-extracted (SRC) and \index{object-extracted structure} object-extracted (ORC) relative clauses as in Example~(\ref{ex:staub10}).

\begin{exe}
\ex\label{ex:staub10}
\begin{xlist}
  \item The employees that [$_{V}$ noticed] [$_{NP}$ the fireman] hurried across the open field.
  \item The employees that [$_{NP}$ the fireman] [$_V$ noticed] hurried across the open field.
\end{xlist}
\end{exe}

A remarkably consistent fact across many languages is that SRCs are easier to comprehend than ORCs \cite[e.g.,][]{KingJust1991,GibsonDesmetGrodner2005,TraxlerMorrisSeely2002,SchriefersFriedericiKuhn1995,Frazier1987a,KwonGordonLee2010}.\footnote{There seem to be some exceptions to this generalization: Basque \citep{carreiras2010subject}, Hindi \citep{VasishthLewis2006}, and Mandarin Chinese \citep{HsiaoGibson2003} have been argued to either show an ORC advantage, or show no indication of a processing difference.}

Two major theoretical explanations of this difference in comprehension difficulty are based on memory processes, and on expectation-based processing. Memory-based accounts \citep{Gibson1998,Gibson2000,grodner,LewisVasishth2005,LewisVasishthVanDyke2006,McElreeForakerDyer2003} predict difficulty in ORCs due to the increased distance between the embedded verb \textit{noticed} and its subject \textit{employees} or memory interference with the intervening noun \textit{fireman}.
 
According to expectation-based explanations \citep{Hale2001,Levy2008,GennariMacDonald2009,MitchellCuetosCorley1995}, SRCs are easier to process because their structure is more frequent and more regular than that of ORCs. For most languages, both accounts predict a preference for the SRC. However, for Mandarin relative clauses the two accounts make opposing predictions, which might be the cause for the long-lasting debate about which relative clause type is easier to process in Mandarin: Expectation predicts a subject-relative preference here, too, because SRCs are also more frequent in Mandarin. But as Mandarin is an SVO language and its RCs are pre-nominal, the dependency is more distant in the SRC than in the ORC. Hence, a preference for ORCs is predicted by memory-based accounts in Mandarin.\footnote{The case of Mandarin has been investigated closely \citep{gibsonwu,LinBever2006,HsiaoMacDonald2013,VasishthChenLi2013,JagerChenLi2015,WuKaiserVasishth2017}; in our opinion, much of the evidence for the surprising ORC advantage in Mandarin is probably due either to Type M errors, confounded designs, incorrect statistical analyses, or other sources of bias that  skewed the conclusions \citep{Vasishth:MScStatistics}.}

In his eye-tracking experiment, \cite{Staub2010a} found increased difficulty at two positions in the ORC (\ref{ex:staub10}b): At the embedded noun phrase \textit{the fireman}, an increased probability was found of outgoing first-pass regressions (FPRP), and at the embedded verb \textit{noticed} increased reading times were observed in gaze duration, first-fixation duration (FFD), and go-past time (regression-path duration, RPD). 
\cite{Staub2010a} interpreted the results as evidence for both memory-based and expectation-based explanations for difficulty in ORCs. The difficulty on \emph{the fireman} can be explained by expectation: On seeing the embedded subject noun phrase of the ORC, the expectation for an SRC is violated, which would cause surprise. An effect at \textit{noticed} is predicted according to memory-based accounts due to difficulty integrating the distant dependency with the subject.
This analysis was supported by the fact that the two observed effects were qualitatively different: Elevated reading times at the ORC verb are compatible with memory-based explanations that predict an increased memory access latency for distant dependencies. More frequent regressions from the subject noun in the ORC  could indicate surprise, causing the reader to interrupt reading and potentially revisit previously-read material that led to the now-falsified expectation.

The \cite{Staub2010a} experiment is a good test case for the model developed in the previous chapter for two reasons: First, relative clauses are possibly the best-studied constructions in psycholinguistics and the subject-relative preference is widely believed to be a cross-linguistically reliable (replicable) result. Second, the cue-based retrieval model not only contains a memory component but also an incremental serial parsing mechanism that commits to one structure at a time. This implicates an expectation component by pre-building the necessary structure, which has to be revised when the parser is garden-pathed. At least in English relative clauses, garden-pathing happens if we encounter an ORC structure when an SRC is expected.\footnote{Ranked parallel parsers do not assume a revise-on-garden-path mechanism in the sense of \cite{FrazierRayner1982}, but react by re-ranking possible parses, a process which could be time-consuming. If re-ranking takes times, increased difficulty is predicted just as in the case of a serial parser. However, re-ranking might make different predictions for regressive eye movements than serial parsers: because parallel parsers maintain multiple parses in parallel, it is not necessary to revise previous commitments and, thus, a parallel parsing mechanism may predict only inflated fixations times, and no difference in regression probability.} 
Therefore, a natural next step is to implement a mechanism that interfaces moments of garden-path with eye movement behavior. This is pursued in the next subsection.


\subsection{Simulation: Modelling the Staub (2010) data}
When integrating a word that is ambiguous in its function in the current parse, an incremental serial parser commits to one possible continuation. By doing so, the parser creates a kind of \emph{expectation} for subsequent words to conform with this commitment. If the upcoming material cannot coherently be integrated, the earlier decision has to be reanalyzed.
When encountering a relative pronoun, the \cite{LewisVasishth2005} parser acts according to a subject preference, always creating an SRC structure.
When finding an NP to follow the relative pronoun (the subject NP of an ORC), the parsing rules execute a \emph{reanalysis} of the pre-built \index{SRC structure} SRC structure as a \emph{gapped} \index{ORC structure} ORC structure. In particular, this means that the relative pronoun is made available as a filler that can be retrieved as the object at the ORC verb. For that, an additional retrieval of the relativizer DP has to be executed. 

This revision process, which occurs at the ORC subject NP, is compatible with the assumptions of expectation-based theories, namely that at this point surprise occurs and expectations have to be revised. The expectation in this case is the syntactic structure built so far, which has to be altered in order to consistently incorporate the present input.

With the Time Out extension presented in the previous chapter, elevated reading times and occasional regressions would be predicted at the ORC subject, because the additional rule firing and retrieval of the revision process delays the completion of the integration at this point. This would be the case if we assume a ``slow integration failure'', as \cite{ReichleWarrenMcConnell2009} call it. However, the case of the SRC/ORC revision is better described as what \cite{ReichleWarrenMcConnell2009} call ``fast integration failure''. In contrast to slow failure, where  the integration time simply takes longer than usual, at a revision an error is encountered immediately, with the result that previous material has to be revisited. 
According to the definition of \cite{ReichleWarrenMcConnell2009}'s fast failure, an immediate attention shift is triggered towards the potential cause of the error. In the SRC/ORC revision, the cause of the error (the violation of expectation) is the structural decision made at the relative pronoun.
It is reasonable to assume an attention shift towards previous material in the case of a revision but not in the case of a regular retrieval, because, in a revision, previously attached material changes its role, i.e., its position in the sentence structure. 

The Reanalysis interface is defined as follows:
\begin{description}
  \item[Reanalysis] Any rule that changes the attachment of a previously created syntactic object in memory triggers an immediate attention shift towards a point in the sentence that is related to the object in question. 
\end{description}

The specific target of the attention shift and the potentially accompanying regression needs further research. As \cite{MalsburgVasishth2011,MalsburgEtAl2015} and others before them \citep{Meseguer2002,greenmitchellJML06} have pointed out, it is still unclear how regression paths behave in reanalysis.
Consequently, for the current simulations, the model is restricted to specifying the source of the regression and not the target. A least-commitment implementation was chosen that selects any target to the left of the current fixation.

\subsection{Results}

\begin{figure}[tb]
  \centering
<<staub10modelRT,echo=FALSE, warning=FALSE, error=FALSE, message=FALSE,fig.height=3,fig.width=5>>=

require(em2)
require(reshape)
require(ggplot2)

## Staub 2010 simulated data, unaggregated:
load("chapters4to6_simulations/chapter_6/staub10/m.RData")
mlt <- melt(m, id=c("roi","roi2","pos","cond"), 
            measure=c("FFD","FPRT","TFT","RPD","RBRT"), 
            na.rm = TRUE)
## by roi
cst <- cast(subset(mlt,value>0), variable+roi+cond ~ ., function(x) c(M=mean(x), SE=sd(x)/sqrt(length(x)), N=length(x), CI=ci(x)))
means.t <- cst
## by roi2
cst <- cast(subset(mlt,value>0), variable+roi2+cond ~ ., function(x) c(M=mean(x), SE=sd(x)/sqrt(length(x)), N=length(x), CI=ci(x)))
means.t2 <- cst
#
## PROBABILITIES:
mlt <- melt(m, id=c("roi","roi2","pos","cond","iteration"), measure=c("refix","reread","fp_reg","skip"), na.rm=T)
## by roi
cst <- cast(mlt, variable+roi+cond ~ ., function(x) c(M=mean(x), N=length(x)))
cst$SE <- sqrt(cst$M*(1-cst$M))/sqrt(cst$N)
cst$CI.lower <- cst$M + qt(.025, df=cst$N-1) * cst$SE
cst$CI.upper <- cst$M + qt(.975, df=cst$N-1) * cst$SE
means.p <- cst
## by roi2
cst <- cast(mlt, variable+roi2+cond ~ ., function(x) c(M=mean(x), N=length(x)))
cst$SE <- sqrt(cst$M*(1-cst$M))/sqrt(cst$N)
cst$CI.lower <- cst$M + qt(.025, df=cst$N-1) * cst$SE
cst$CI.upper <- cst$M + qt(.975, df=cst$N-1) * cst$SE
means.p2 <- cst
#
means <- rbind(means.t,means.p)
means2 <- rbind(means.t2,means.p2)
colnames(means)[3] <- "Condition"
colnames(means2)[3] <- "Condition"
#
means2$roi2 <- factor(means2$roi2, labels=c("REL", "RC-NP", "RC-V", "V", "Other"))
means$Condition <- factor(means$Condition, levels=c("SRC","ORC"))
means2$Condition <- factor(means2$Condition, levels=c("SRC","ORC"))

## actual data from Table, p 75 of Staub 2010
d <- read.table("chapters4to6_simulations/chapter_6/staub10/staub10-data.txt", header=TRUE)
colnames(d)[1:2] <- c("variable", "Condition")
d$Condition <- factor(d$Condition, levels=c("SRC","ORC"))
# head(d)
d2 <- d
d2$roi2 <- as.vector(d2$roi)
d2$roi2[d2$roi2%in%c("DET","N")] <- "RC-NP"
d2 <- cast(d2, variable+roi2+Condition ~ ., value="data", mean)
colnames(d2)[4] <- "data"
# head(d2)
#
means <- merge(means, d, by=c("variable","Condition","roi"), all.x=TRUE)
means2 <- merge(means2, d2, by=c("variable","Condition","roi2"), all.x=TRUE)
# head(means2)

dodge1 <- position_dodge(0.07)

crit_dat<-droplevels(subset(means2, (variable%in%c("FFD","FPRT","RPD") & roi2%in%c("RC-NP","RC-V","V"))))

## combine data and model
model<-crit_dat[,c(1,2,3,4,7,8)]
model$data_model<-"model"

data<-crit_dat[,c(1,2,3,9)]
colnames(data)[4]<-"M"
data$CI.lower<-data$CI.upper<-NA
data$data_model<-"data"

dat_model<-rbind(model,data)

## if needed later:
FFD<-subset(dat_model,variable=="FFD")
FPRT<-subset(dat_model,variable=="FPRT")
RPD<-subset(dat_model,variable=="RPD")

p<-ggplot(crit_dat, 
  aes(roi2, M, 
      linetype=Condition, group=Condition)) + 
  geom_line(colour="#999999", aes(y=data), 
            position=dodge1) + 
  geom_point(colour="#999999", size=2, shape=2, aes(y=data), position=dodge1) + 
  geom_line(position=dodge1) + 
  geom_point(position=dodge1) + 
  geom_errorbar(aes(max=CI.upper, 
                    min=CI.lower, width=0), position=dodge1) + 
  xlab("Region") + 
  ylab("Mean duration in ms") + 
  scale_linetype_manual("Condition", values = c(1,2)) + scale_shape_manual("Data", values = c(2,1)) + 
  facet_grid(. ~ variable, scales="free")

p<-p+theme_bw()
p+theme(legend.position="top",legend.key=element_blank(),panel.grid.minor.y=element_blank(),panel.grid.major.y=element_line(color="gray80"),panel.grid.minor.x=element_blank(),panel.grid.major.x=element_blank())
@
\caption[Model predictions for reading times in subject- and object-relative clauses.]{Model predictions for reading times in subject- and object-relative clauses at the relativel clause NP (RC-NP), the relative clause verb (RC-V), and the  main verb (V). The dependent measures are first fixation durations (FFD), first-pass reading time (FPRT), and regression path duration (RPD). The data are from Table 1 of Staub (2010), and are shown in gray. Error bars represent 95\% confidence intervals from the model simulations. This is a zero-parameter fit: no parameter estimation was done to fit the model to the data.} \label{fig:staubmodel:rt}
\end{figure}

\begin{figure}[tb]
  \centering
<<staub10modelREG,echo=FALSE, warning=FALSE,error=FALSE, message=FALSE, fig.width=5,fig.height=4, out.width='0.7\\textwidth'>>=
# ,out.width='0.45\\textwidth'

## REGRESSION PROB
dodge2 <- position_dodge(width=.9)

(p.reg <- ggplot(
  droplevels(subset(means2, (variable%in%c("fp_reg") & roi2%in%c("RC-NP","RC-V","V")))), aes(roi2, M, fill=Condition))
# + geom_point(colour="gray70", size=3, aes(y=data), position=dodge2)
+ geom_bar(stat="identity", position=dodge2)
# + geom_text(colour="black", size=4, label="- - - - - -", aes(y=data), position=dodge2)
+ geom_point(colour="black", size=3, shape=2, aes(y=data), position=dodge2)
+ geom_errorbar(aes(max=CI.upper, min=CI.lower, width=0), position=dodge2)
+ xlab("")
+ ylab("First-pass regression proportion")
+ scale_fill_manual("Condition", values = c("gray40","gray70"))
+ coord_cartesian(ylim=c(-0.01,.7))
+ theme_bw()
+ theme(legend.position=c(.8,.8)  
#+ theme(legend.position="right"
  ,legend.key=element_blank()
  ,panel.grid.minor.y=element_blank()
  ,panel.grid.major.y=element_line(color="#999999")
  # ,panel.grid.minor.x=element_line(color="gray60")
  ,panel.grid.minor.x=element_blank()
  ,panel.grid.major.x=element_blank()
  )
)

@
\caption[Predicted first-pass regressions from the model for subject- and object-relative clauses.]{Model predictions for first-pass regressions in subject- and object-relative clauses at embedded NP, embedded verb, and main verb. The data are from Staub (2010) and are shown as triangles. Error bars represent 95\% confidence intervals from the model simulations. This is a zero-parameter fit: no parameter estimation was done to fit the model to the data.} \label{fig:staubmodel:reg}
\end{figure}

Simulations were performed using the parameter values that have been estimated with the Time Out model on the Potsdam Sentence Corpus. Each sentence of Example~(\ref{ex:staub10}) was run $200$ times.

Figure~\ref{fig:staubmodel:rt} shows mean reading times for first-fixation duration, gaze duration, and go-past time in comparison with the data (in gray). 
In first fixations and gaze durations, the \cite{Staub2010a} data shows reading time differences only on the embedded verb, the \index{ORC} ORC being slower than the \index{SRC} SRC. In go-past times, more regressions occur at this point in the ORC than the SRC.

Qualitatively, the model predicts these patterns exactly in all three eye movement measures. Numerically, there is a remarkably close fit in first-fixation durations. Also in gaze and go-past times, the predictions are numerically in a similar range as the data, but the SRC reading times are underestimated in the predictions by $50$ to $100$ milliseconds.

First-pass regression proportions are shown in Figure~\ref{fig:staubmodel:reg}. The empirical means are from Table 1 of \cite{Staub2010a} and are indicated by the black triangles. They show an increased regression rate in ORCs at the relative clause NP. 
As for reading times, the model predictions for regressions pattern with the data. The model predicts no first-pass regressions for the SRC at the NP or the verb inside the relative clause; by contrast, the data show regressions in the SRC condition. The model predicts the major empirical finding in \cite{Staub2010a} of an increased regression rate at the relative clause NP for the ORC. It does, however, additionally predict a slight increase in regressions at the verb in ORC vs SRC conditions, which has not been found in the data.

\subsection{Discussion}
The simulation tested the model on specific predictions for English relative clauses. No parameter fitting to the data was performed. The model with parameters as estimated on the Potsdam Sentence Corpus generated predictions that  qualitatively reproduce the theoretically important patterns in the data. The Time Out interface developed in the previous chapter predicts memory-based differences and magnitudes in three different reading time measures in three regions remarkably well. This shows that the parameter estimates that were computed for the Potsdam Sentence Corpus of German can be used for experimentally controlled comparisons of memory-related complexities in English. The newly introduced Reanalysis interface II, which is similar to \cite{ReichleWarrenMcConnell2009}'s rapid integration failure, correctly predicts increased first-pass regressions due to an invalidated prediction. 

In sum, \cite{Staub2010a} proposes that the qualitatively distinct effects on the RC noun phrase and the RC verb indicate distinct sources of difficulty, namely expectation and memory, respectively. These two sources of difficulty can be explained within the cue-based retrieval model, without recourse to additional modelling assumptions involving \index{probabilistic predictions} probabilistic predictions. Note that the mechanism of interaction in the model is in both cases the same: The parser intervenes in the forward movement of the eyes, triggering a regression. However, the timing differs in that the intervention happens earlier for violated expectations: memory-induced time outs are triggered after recognition of the next word, but reanalysis regressions are triggered as soon as the parser detects that the input is unexpected. Due to this timing difference, the time out regressions on the verb are cancelled most of the time before they are executed, because the memory processes are not delayed long enough before normal reading is resumed. The planning and cancelling of regressions thus leads to inflated reading times. By contrast, reanalysis is triggered earlier and the parser has to perform additional actions for revising structure, which leaves enough time for the regression to be completed. 
This is an example of a simple mechanism producing complex behavior: The same mechanism --- triggering a regression ---  produces qualitatively different effects under certain circumstances, based on the relative timing between parsing and eye-movement control. One aspect that is not captured in the model is the effect of eye-movement control constraints on regressive eye movements \citep{EngbertNuthmannRichter2005}. In order to incorporate the effect of eye-movement control, a comprehensive integration of a parsing model and an eye-movement control model like SWIFT is needed. At the time of writing, this research is ongoing \citep{Rabe2019}.

The model predictions thus capture the major findings of Staub's study. However, the model is very simplified and consequently deviates in its predictions in some ways from the data.
First, a subject-relative preference is hard-coded into the model, whereas human readers might also expect ORCs in some cases. The underestimation of SRC gaze and go-past durations in the model would be less significant with a probabilistic decision between pursuing an SRC or ORC structure.\footnote{In ACT-R, it is possible to learn the utilities of parsing rules over a number of trials, based on the number of successful applications. It would be necessary to simulate reading of a corpus that represents the natural distributions of relative clauses and similar structures.}

Second, \index{first-pass regression} first-pass regression proportions are predicted to be slightly increased on the RC verb in the ORC, although the only empirical effect reported was on the RC noun phrase. This indicates that not all of the time out regressions that are triggered at this region were cancelled but some had enough time to be executed, meaning that the model predicts memory-based regressions on the verb which are not supported by Staub's data. However, the biggest effect in the predictions of first-pass regressions is on the ORC subject, consistent with the data.

Third, expectation-based first-pass regressions are only predicted on the determiner of the ORC subject, not on the noun. In the data, however, the effect is found at the determiner \emph{and} the noun. This is most likely a spill-over effect due to delayed parsing in some trials, which is not predicted in this case by the model. It is possible in the model that parsing processes on word n influence the fixation time on word n+1: This happens when a time out is initiated while the eyes have already moved on to word n+1. However, this does not happen in the simulation, because the detection of the validated expectation is instantaneous, meaning it is part of the first parsing production firing at the determiner. Hence, reanalysis is always initiated before the time-out production can fire. This might be a limitation of the model which will have to be addressed in the future. However, an alternative cause for spill-over effects --- especially, if they span multiple words --- may be that readers delay parsing in order to collect more information before making structural decisions.
A likely mechanism for delaying parsing processes in this way is to hold words temporarily in the articulatory loop \citep{BaddeleyHitch1974,Baddeley2003}. This possibility is explored through a proof-of-concept simulation in \cite{engelmann:phd}.

\section[Modelling underspecification]{Modelling underspecification: The adaptive interaction between parsing, eye-movement control, and working memory capacity}
\label{sec:sim:III}

Several researchers, such as \cite{Traxler2007}, have proposed that individuals with low working-memory capacity might build less structure than individuals with high working-memory capacity. For example, in sentences like \textit{The sister of the writer that had blond hair arrived this morning}, it isn't clear whether the sister had blond hair or the writer. One proposal is that low-capacity individuals might simply not commit to one or another attachment, leaving the structure underspecified. This proposal implies two distinct strategies for low- vs.\ high-capacity individuals. In this section, we show how our model, which integrates eye-movement control and parsing processes, can explain such underspecification processes as a function of capacity differences. The model assumes that underspecification occurs as a consequence of low-capacity readers taking longer on average to complete an attachment. Whether an attachment completes or not in a particular trial depends on whether the attachment can be completed fast. If a reader (such as a low-capacity reader) takes longer to read, this will non-deterministically lead to the cancellation of the attachment process, leading to underspecification being observed in some trials. The interesting aspect of the model is that underspecification emerges as a side-effect of the interplay between ``normal'' reading and working-memory capacity. 

\subsection{Good-Enough Parsing}
\index{good-enough processing}
The good-enough approach to sentence processing \citep{FerreiraFerraroBailey2002,SanfordSturt2002} suggests that readers strategically adapt their efforts to task demands with the consequence of sometimes not arriving at a complete parse. 
This implies that readers leave some structural relations underspecified in order to save processing time if the effort does not seem necessary for the task at hand.

An example of underspecification is the finding that some ambiguous attachment relations are read faster than their unambiguous counterparts. For example, \cite{TraxlerPickeringClifton1998} and \cite{Traxler2007} studied sentences like (\ref{ex:traxler07}c) that were globally ambiguous with regard to the attachment of the relative clause to one of the noun phrases \textit{sister} or \textit{writer} and compared them to sentences where the relative clause was unambiguously attached \emph{high} (\ref{ex:traxler07}a) or \emph{low} (\ref{ex:traxler07}b).

\begin{exe}
\ex\label{ex:traxler07}
\begin{xlist}
\item The writer of the letter/ that had/ blonde hair/ arrived this/ morning.
\item The letter of the writer/ that had/ blonde hair/ arrived this/ morning.
\item The sister of the writer/ that had/ blonde hair/ arrived this/ morning.
\end{xlist}
\end{exe}

Both studies found an ambiguity advantage at the disambiguation region \textit{blonde hair}. An analysis of individual working memory capacity in \cite{Traxler2007} revealed that \index{high-capacity readers} high-capacity readers showed an expected preference for high attachment (NP1). \index{low-capacity readers} 
Low-capacity readers, in contrast, showed no such preference. According to \cite{Traxler2007}, it might be that low-capacity readers leave the attachment underspecified or the selection of the attachment site is the product of a balance between a high-attachment preference and recency.

Building on work by \cite{JustCarpenter1992}, \cite{macdonald1992working}, among others, 
\cite{KemperCrowKemtes2004} studied a main clause/relative clause ambiguity such as (\ref{ex:kemper04}). 

\begin{exe}
\ex\label{ex:kemper04}
\begin{xlist}
\item The experienced soldiers/ warned about the dangers/ before the midnight raid.
\item The experienced soldiers/ warned about the dangers/ conducted the midnight raid.
\item The experienced soldiers/ spoke about the dangers/ before the midnight raid.
\item The experienced soldiers/ who were told about the/ dangers conducted the midnight raid.
\end{xlist}
\end{exe}

In (\ref{ex:kemper04}a) and (\ref{ex:kemper04}b), the role of \textit{warned} is temporarily ambiguous between the main verb and the embedded verb of a reduced relative clause. In (\ref{ex:kemper04}b), the ambiguity is resolved towards the non-preferred reduced-relative reading. In (\ref{ex:kemper04}c), the verb \textit{spoke} unambiguously induces a main verb reading. Kemper and colleagues found the expected difficulty at \textit{conducted} in the non-preferred condition (\ref{ex:kemper04}b). However, in contrast to the studies by Traxler and colleagues, they found no ambiguity advantage. An analysis of working memory differences showed that low-capacity readers had more difficulty resolving the ambiguity, which was indicated by slower reading and higher regression rates at the disambiguating region.

A possible account of when attachments are underpecified and thus an explanation for the difference between Traxler's experiments and \cite{KemperCrowKemtes2004} is offered by \index{construal} \emph{construal} \citep{CarreirasClifton1993,FrazierClifton1997}. This theory differentiates between  ``primary'' and ``nonprimary'' relations, where primary roughly stands for relations that are obligatory for deriving a coherent message (e.g., verbs and their arguments). The attachment of primary relations is always carried out according to garden-path theory \citep{Frazier1987} while the definite attachment of nonprimary relations can be suspended by loosely associating it with the last theta domain \citep{FrazierClifton1997}.

More evidence for construal and the good-enough account \citep{FerreiraFerraroBailey2002,SanfordSturt2002} comes from a self-paced reading study by \cite{SwetsDesmetClifton2008}. Similar to \cite{TraxlerPickeringClifton1998} and \cite{Traxler2007}, Swets and colleagues studied ambiguous relative clause attachments as in Example (\ref{ex:swets08}).

\begin{exe}
\ex\label{ex:swets08}
\begin{xlist}
\item The maid of the princess who scratched herself in public was terribly humiliated.
\item The son of the princess who scratched himself in public was terribly humiliated.
\item The son of the princess who scratched herself in public was terribly humiliated.
\end{xlist}
\end{exe}

\cite{SwetsDesmetClifton2008} manipulated task demands by using either superficial comprehension questions or questions that specifically queried the interpretation of the relative clause. The results showed an ambiguity advantage in (\ref{ex:swets08}a) vs.\ (b) and (c) at the disambiguating reflexive only when questions were superficial, indicating that question type affected the readers preference to leave the RC attachment underspecified. 
In addition, in the condition with questions targeting the relative clause (RC question condition), question response times were elevated for ambiguous sentences. This indicates that even in the RC-question condition the attachment was sometimes left unspecified and had to be resolved during the question answering phase. In the RC-question condition, a disambiguation towards NP1 (\ref{ex:swets08}b) resulted in longest reading times, pointing to a preference towards NP2 in the initial attachment, which had to be revised at the reflexive. For a detailed analysis of the \cite{SwetsDesmetClifton2008} data and the compatibility with assumptions about underspecification, see \cite{LogacevMultiple,LogacevVasishthQJEP2016}.

In related work, \cite{MalsburgVasishth2013} investigated the influence of working memory capacity on the preferences for underspecification.
They conducted an eye-tracking experiment using the stimuli of a Spanish study by \cite{Meseguer2002} and analyzed the participants' individual scanpaths \citep{MalsburgVasishth2011} conditional on working memory capacity. \index{working memory capacity}
In the sentences studied (Example~\ref{ex:malsburg13}), an adjunct (\textit{cuando los directores...}) was temporarily ambiguous between modifying the main verb \textit{dijo} as a temporal adverbial clause (high attachment) or the embedded verb \textit{se levantaran} as a conditional (low attachment). 

\begin{exe}
\ex\label{ex:malsburg13preamble} 
\gll El profesor dijo que los alumnos {se levantaran} del asiento\\
     The professor said that the students {stand up} from seat \\
\glt `The teacher said that the students had to stand up from their seats\dots'
\end{exe}

\begin{exe}
\ex\label{ex:malsburg13} 
\begin{xlist}
\item HIGH
\gll \dots cuando los directores \textbf{entraron} en la clase de m{\'u}usica. \\
\dots when the directors entered in the class of music\\
\glt \dots when the directors entered in the class of music.
\item LOW
\gll cuando los directores \textbf{entraran} en la clase de m{\'u}sica.\\
\dots when the directors entered in the class of music\\
\glt \dots when the directors entered in the class of music.
\item  UNAMB(IGUOUS)
\gll \dots \textbf{si} los directores entraban en la clase de m{\'u}sica.\\
\dots if the directors entered in the class of music\\
\glt \dots if the directors entered in the class of music.
\end{xlist}
\end{exe}

The attachment site was disambiguated at the verb \textit{entraron} (indicative) / \textit{entraran} (subjunctive) towards HIGH (\ref{ex:malsburg13}a) or LOW (\ref{ex:malsburg13}b) attachment, respectively. In the unambiguous (UNAMB) condition (\ref{ex:malsburg13}c), using the word \textit{si} (``if'') instead of \textit{cuando} (``when''/``if'') unambiguously signalled LOW attachment of the adjunct as a conditional.

%\begin{figure}[!htbp]
%  \centering
<<mv13dataambadv,echo=FALSE, include=FALSE,warning=FALSE, error=FALSE, message=FALSE, fig.width=5,fig.height=3, out.width='0.8\\textwidth'>>=
data <- read.table("chapters4to6_simulations/chapter_6/mv13/data-mv13.txt")
data$cond<-factor(data$cond)
levels(data$cond) <- c("HIGH","LOW","UNAMB")
data$amb <- ifelse(data$cond=="UNAMB", "Unambiguous", "Ambiguous")
data$FPRT.c.exp <- exp(data$FPRT.c)
data$wmc <- factor(ifelse(data$pcu >= median(data$pcu), "high WMC", "low WMC"))

## Ambiguity advantage ##
mlt <- melt(data, id=c("amb","wmc"), measure=c("FPRT.c.exp"), na.rm = TRUE)
cst <- cast(subset(mlt,value>0), variable+amb+wmc ~ ., function(x) c(M=mean(x), SE=sd(x)/sqrt(length(x)), N=length(x), CI=ci(x)))
# head(cst)
d.means.t3 <- cst
# There is a mistake in the original paper; this should have been high and then low:
#d.means.t3$wmc <- rep(c("low", "high"),2)
# this is correct:
d.means.t3$wmc <- rep(c("high", "low"),2)
colnames(d.means.t3)[3] <- "WMC"

(pvdmv13rer<-ggplot(aes(amb, M, group=WMC, lty=WMC),#, ymax=509, ymin=460), 
data=subset(d.means.t3,variable=="FPRT.c.exp"))
 + ylab("Mean gaze duration in ms")
 + xlab("")
 + geom_line(position=position_dodge(0.05)) 
 + geom_point(size=3,position=position_dodge(0.05)) 
 + geom_linerange(aes(max=CI.upper, min=CI.lower),position=position_dodge(0.05))
 # + scale_colour_manual("WMC", values=UPpalette(firstcolor="humfak",secondcolor="darkblue")) + scale_x_discrete(expand=c(0.1,0)) + scale_y_continuous(expand=c(0.0,0))
 # + scale_linetype_manual("WMC", values = c(1,3))
 # + scale_color_manual("WMC", values=c("black","gray"))
 + theme_bw()
 + theme(legend.position="right"
  ,legend.key=element_blank()
  ,panel.grid.minor.y=element_blank()
  ,panel.grid.major.y=element_line(color="gray80")
  # ,panel.grid.minor.x=element_line(color="gray60")
  ,panel.grid.minor.x=element_blank()
  ,panel.grid.major.x=element_blank()
  # ,plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), "cm")
  )
)
@
%  \caption[Ambiguity advantage in the pre-verbal region for low-capacity readers in the data of von der Malsburg and Vasishth (2013).]{Ambiguity advantage for low- vs.\ high-capacity readers in the data of von der Malsburg and Vasishth (2013). Shown are gaze durations in the pre-verbal region (\textit{cuando/si los directores}) for ambiguous (a and b) and unambiguous (c) conditions, grouped by high and low working memory capacity as measured by the operation span task.  Error bars represent 95\% confidence intervals.}\label{fig:mv13data:rt}
%\end{figure}


\begin{figure}[!htbp]
\centering
<<mv13datarer, include=TRUE,echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, fig.width=6,fig.height=4, out.width='0.7\\textwidth'>>=
## mean rereading
mlt <- melt(data, id=c("cond","wmc"), measure=c("isa","acc"), 
            na.rm=TRUE)
cst <- cast(mlt, variable+cond+wmc ~ ., function(x) c(M=mean(x), N=length(x)))
cst$SE <- sqrt(cst$M*(1-cst$M))/sqrt(cst$N)
cst$CI.lower <- cst$M + qt(.025, df=cst$N-1) * cst$SE
cst$CI.upper <- cst$M + qt(.975, df=cst$N-1) * cst$SE
# head(cst)
d.means.p2 <- cst
#levels(d.means.p2$wmc)
d.means.p2$wmc <- c("high", "low")
colnames(d.means.p2)[3] <- "WMC"

(pvdmv13rereading<-ggplot(aes(cond, M, fill=WMC, group=WMC), data=droplevels(subset(d.means.p2, (variable%in%c("isa"))))) 
  + ylab("Rereading proportion") 
  + xlab("")
  + scale_fill_manual(values=c("gray40","gray70")) 
  + scale_x_discrete(expand=c(0.1,0)) 
  + scale_y_continuous(expand=c(0,0), limits=c(0,.31))
  + geom_bar(position=position_dodge(0.9), stat="identity") 
  + geom_linerange(aes(max=CI.upper, min=CI.lower), position=position_dodge(0.9))
 + theme_bw()
 + theme(legend.position="right"
  ,legend.key=element_blank()
  ,panel.grid.minor.y=element_blank()
  ,panel.grid.major.y=element_line(color="gray80")
  # ,panel.grid.minor.x=element_line(color="gray60")
  ,panel.grid.minor.x=element_blank()
  ,panel.grid.major.x=element_blank()
  # ,plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), "cm")
  )
)
@
  \caption[Proportions of sentence rereading by working memory capacity in the data of von der Malsburg and Vasishth (2013).]{Proportions of sentence rereading in the data of von der Malsburg and Vasishth (2013) for ambiguous high (a), low (b), and unambiguous (c) conditions, grouped by high and low working memory capacity. Error bars represent 95\% confidence intervals.}
  \label{fig:mv13data:rer}
\end{figure}

Von der Malsburg and Vasishth investigated the ambiguity advantage in the pre-verbal region \textit{cuando los directores}. For this design, \cite{Traxler2007} would predict that low-capacity readers underspecify more often. Support for the \cite{Traxler2007} prediction  comes from the proportions of rereading of the whole sentence after seeing the disambiguating region (see Figure~\ref{fig:mv13data:rer}). 
For high-capacity readers, the rereading proportion was highest for condition (a), where the disambiguation was towards the more distant high attachment. In contrast, low-capacity readers showed no statistically significant difference in rereading proportions between conditions. According to \cite{MalsburgVasishth2013}, this suggests that high-capacity readers complete the attachment of the adjunct more often and thus have to reanalyze later, whereas low-capacity readers have no need for reanalysis because they tend to leave the attachment unspecified in the ambiguous conditions. The reason for reanalysis occurring predominantly in the high-attachment condition is that the conditional interpretation with a low attachment is initially preferred with the word \textit{cuando}.

The evidence summarized above suggests that ambiguous attachment relations are strategically underspecified, and that readers with
\index{working memory capacity}
low working memory capacity leave an attachment underspecified more often than high-capacity readers do. 
It is not clear, however, how this adaptation works. Is it necessary to assume that low-capacity readers use a different parsing strategy, or can the difference be explained by a common mechanism? So far, there exists no detailed, computationally implemented model of the good-enough account that could clarify this issue.

Here, we propose that underspecification is a consequence of a single strategy that aims for \index{uninterrupted reading} uninterrupted reading whenever possible.
In particular, for nonprimary relations in the sense of \cite{FrazierClifton1997}, an attachment is treated as non-obligatory and completed only if enough time is left before the next word is ready to be integrated.
Thus, underspecification is not a deterministic process but dynamically adapts to the relative timing of the attachment process and autonomous low-level eye movement processes. If an attachment is easy and proceeds fast in relation to the ``default'' reading speed (in the sense of \emph{uninterrupted} reading), the attachment is completed. If, however, the attachment could not be made without interrupting the progress of the eyes, it is abandoned in a trade-off with reading speed. 
An influence of working memory differences is predicted by the assumption that, compared to high-capacity readers, low-capacity readers take longer on average to complete the attachment, resulting in more cancellations that leave the attachment underspecified.

\subsection{Simulation: Modelling the von der Malsburg and Vasishth (2013) experiment}

Here, we report an implementation of the model that defines \emph{good-enough} parsing as the result of an interaction between the parser and eye-movement control, as explained above. For non-obligatory relations, the time to attach is constrained by the time needed by saccade programming and low-level processes to identify the next word:

\begin{description}
  \item[Underspecification Interface] \index{Underspecification Interface} For relations with low utility, an attachment attempt is aborted as soon as the next word is ready for integration, so that reading proceeds uninterrupted.
\end{description}

This implementation naturally predicts an influence of working memory capacity. Here, memory capacity is implemented in terms of the  goal buffer source activation parameter $W$ in ACT-R.
Working memory capacity has been modelled in this way before: \cite{DailyEtAl2001} modelled individual differences in the digit-span task \citep{LovettRederLebiere1999} in ACT-R by manipulating the source activation $W$ for the goal buffer.  
In the context of language comprehension, \cite{VanRijVanRijnHendriks2013} used this method by manipulating $W$ for modelling individual differences in pronoun interpretation.
The amount of activation $W$ is equally distributed between sources $j$ in the goal buffer, i.e., the chunks that spread activation to related memory items. Thus, the value of $W$ defines how strongly relevant information from the goal buffer is used for memory retrieval. Therefore, higher values of $W$ improve speed and accuracy of retrieval processes. In this way, the term working memory capacity \index{working memory capacity} is defined as a measure of how well an individual is able to separate information in memory that is relevant to the current task from currently irrelevant information --- a kind of focusing of cognitive attention.
% !!! This means that activation spread is less when there are more slots in the goal buffer (filled slots, those with nil do not count) !!!

As an example simulation, the \cite{LewisVasishth2005} model was extended with parsing rules for sentence constructions as used by \cite{MalsburgVasishth2013} and defined the adjunct attachment as non-obligatory.
In particular, when attaching the adjunct, the parser creates the structure for the adjunct clause and then signals that integration is complete, so that no time-out rule will fire. It then attempts to retrieve both potential attachment sites. 
This process is faster and more accurate for high-capacity readers because, with a high source activation $W$, the retrieval cues  
activate the correct retrieval targets more strongly.
Thus, high-capacity readers are predicted by the model to underspecify less often than low-capacity readers.

An important observation here is that the mechanism is the same for all readers: As soon as the next word is ready for integration, an ongoing attachment process is abandoned.  
Following \cite{SwetsDesmetClifton2008}, an abandoned attachment is not corrected later in the sentence. However, if an attachment was made, disambiguating information that contradicts the attachment decision leads to a repair operation, triggering a regression towards the beginning of the sentence (this assumption is based on the finding by von der Malsburg and Vasishth (2013) that repair processes tend to trigger re-reading of a sentence).

Sixty participants were simulated reading the three attachment conditions of \cite{MalsburgVasishth2013}, HIGH, LOW, and UNAMB, 20 times each.
EMMA parameters and the latency factor were left at values estimated in the simulations involving the Potsdam Sentence Corpus evaluation. Other parameters were set to the values used in \cite{LewisVasishth2005}. However, two parameters were changed for this simulation. First, \emph{Mismatch penalty} MP was set to \texttt{NIL} to switch off partial matching. This was done in order to reduce interference and misretrievals as this was not relevant here. Second, the \emph{maximum associative strength} parameter MAS had to be increased from $1.5$ to $3.5$ in order to have enough spreading activation from the goal buffer for differences in $W$ to have an effect.
For simulating individual differences in working memory capacity, the method of \cite{DailyEtAl2001} was used to randomly assign to $W$ a value drawn from a normal distribution with mean $=1.0$ and standard deviation $=0.25$. 

\subsection{Results}

<<mv13modelprep, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE>>=
load("chapters4to6_simulations/chapter_6/mv13/sim-etm.RData")
#
##
## ADDITIONAL MEASURES
##
m <- etm
m$fp_reg <- ifelse(m$RBRC>0,1,0) ## first-pass regression prob
m$skip<-ifelse(m$FPRT==0,1,0)  ## skipping prob
m$reread<-ifelse(m$RRT>0,1,0)  ## rereading prob
m$refix <- ifelse(m$FPRT>m$FFD,1,0)  ## re-fixation prob
#m$onefix <- ifelse((m$SFD>0 & m$FFP==1),1,0)
# head(m)
m$skip.prev <- factor(c(m$skip[dim(m)[1]],m$skip[1:dim(m)[1]-1]))
m$skip.next <- factor(c(m$skip[2:dim(m)[1]], m$skip[1]))
m$reg.prev <- factor(c(m$fp_reg[dim(m)[1]],m$fp_reg[1:dim(m)[1]-1]))
m$reg.next <- factor(c(m$fp_reg[2:dim(m)[1]], m$fp_reg[1]))
# head(m)

m$amb <- ifelse(m$cond=="UNAMB", "Unambiguous", "Ambiguous")

##
## OTHER INFO
##
## encoding times ##
enc <- read.table("chapters4to6_simulations/chapter_6/mv13/enctimes.txt", header=TRUE)
# head(enc,20); dim(enc)
colnames(enc) <- c("exp","iteration","cond","roi","word","enc","ecc","freq1")
# Only store first encoding of each word
enc$trialroi <- paste(enc$exp, enc$iteration, enc$cond, enc$roi); #length(enc$trialroi)
enc$dupl <- duplicated(enc$trialroi)
enc1 <- subset(enc, !dupl); # head(enc1[-c(9,10)]); dim(enc1)
m <- merge(m, enc1[-c(9,10)], by=c("exp","iteration","cond","roi"), all.x=TRUE)

## time outs ##
tmo <- read.table("chapters4to6_simulations/chapter_6/mv13/timeouts.txt", header=TRUE)
# head(tmo)

## attachment times ##
retr <- read.table("chapters4to6_simulations/chapter_6/mv13/attachments.txt", header=TRUE)
colnames(retr) <- c("exp","iteration","cond","roi","word","retr")
retr$retr <- as.integer(retr$retr)

# Only store first encoding of each word
retr$trialroi <- paste(retr$exp, retr$iteration, retr$cond, retr$roi); #length(retr$trialroi)
retr$dupl <- duplicated(retr$trialroi)
retr1 <- subset(retr, !dupl); # head(retr1); dim(retr1)
# head(retr1,20)
# head(retr[-c(5,7,8)])
m <- merge(m, retr1[-c(5,7,8)], by=c("exp","iteration","cond","roi"), all.x=TRUE, all.y=FALSE)
# head(m)
m$retr[is.na(m$retr)] <- 20

## trial messages
msg <- read.table("chapters4to6_simulations/chapter_6/mv13/trialmessages.txt", header=F)
colnames(msg) <- c("exp","iteration","cond","pos","word","var","val")
msg <- reshape(msg, idvar = c("exp","iteration","cond","pos","word"), timevar="var", direction="wide")
msg$attached <- ifelse(is.na(msg$val.attachment),0,1)
msg$timeout <- ifelse(is.na(msg$val.timeout), 0, 1)
msg$revision <- ifelse(!is.na(msg$val.revision), 1, 0)
msg$s.reread <- ifelse(msg$val.regression=="reread", 1, 0)
msg$s.reread[is.na(msg$s.reread)] <- 0
msg$fail <- 0
msg$fail[msg$val.fail=="T"] <- 1
# head(msg) # summary(msg)


trialinfo <- unique(m[1:3])
trialinfo <- merge(trialinfo, subset(msg[c('exp','iteration','cond','attached')], attached==1), by=c("exp","iteration","cond"), all.x=TRUE, all.y=TRUE)
trialinfo <- merge(trialinfo, subset(msg[c('exp','iteration','cond','revision')], revision==1), by=c("exp","iteration","cond"), all.x=TRUE, all.y=TRUE)
trialinfo <- merge(trialinfo, subset(msg[c('exp','iteration','cond','s.reread')], s.reread==1), by=c("exp","iteration","cond"), all.x=TRUE, all.y=TRUE)
trialinfo <- merge(trialinfo, subset(msg[c('exp','iteration','cond','fail')], fail==1), by=c("exp","iteration","cond"), all.x=TRUE, all.y=TRUE)
trialinfo[is.na(trialinfo)] <- 0
trialinfo <- merge(trialinfo, subset(msg[c('exp','iteration','cond','val.attachment')], !is.na(val.attachment)), by=c("exp","iteration","cond"), all.x=T, all.y=T)
trialinfo <- droplevels(trialinfo)
# head(trialinfo); summary(trialinfo)

## subject info ##
subjects <- read.table("chapters4to6_simulations/chapter_6/mv13/subjects.txt", header=FALSE)
colnames(subjects) <- c("exp","subj","ga")
# subjects$wmc <- as.factor(ifelse(subjects$ga>1,"highWMC","lowWMC"))
#subjects$wmc <- as.factor(ifelse(subjects$ga<1,"lowWMC","highWMC"))
median_ga <- median(subjects$ga)
mean_ga <- mean(subjects$ga)
subjects$wmc <- factor(ifelse(subjects$ga >= median(subjects$ga), "high WMC", "low WMC"))
# summary(subjects$wmc)
ga_means <- aggregate(subjects$ga, by=list(subjects$wmc), mean)$x
trialinfo <- merge(trialinfo, subjects, by=c("exp"), all.x=T, all.y=T)

## Accuracy ##
trialinfo$acc <- 0
trialinfo$acc[trialinfo$cond=="HIGH" & trialinfo$val.attachment=="high"] <- 1
trialinfo$acc[trialinfo$cond=="HIGH" & trialinfo$revision==1] <- 1
trialinfo$acc[trialinfo$cond=="LOW" & trialinfo$val.attachment=="low"] <- 1
trialinfo$acc[trialinfo$cond=="LOW" & trialinfo$revision==1] <- 1
trialinfo$acc[trialinfo$cond=="UNAMB" & trialinfo$val.attachment=="low"] <- 1
trialinfo$acc[trialinfo$cond=="UNAMB" & trialinfo$revision==1] <- 1
trialinfo$acc[trialinfo$fail==1] <- 0

trialinfo$acc2 <- trialinfo$acc
trialinfo$acc2[trialinfo$attached==0 & trialinfo$fail==0 & trialinfo$cond=="HIGH"] <- .25
trialinfo$acc2[trialinfo$attached==0 & trialinfo$fail==0 & trialinfo$cond!="HIGH"] <- .75
trialinfo$acc2[trialinfo$fail==1] <- .5

#dim(m)
m <- merge(m, trialinfo, by=c("exp","iteration","cond"), all.x=TRUE)
#dim(m)

colnames(m)[4] <- "pos"
m <- merge(m, msg[c(1:4,12)], by=c("exp","iteration","cond","pos"), all.x=TRUE)
m$timeout[is.na(m$timeout)] <- 0
#head(m)

m$enc.prev <- c(m$enc[dim(m)[1]],m$enc[1:dim(m)[1]-1])
m$enc.next <- c(m$enc[2:dim(m)[1]], m$enc[1])

##
## REGIONS OF INTEREST
##
#m$roi <- as.vector(as.numeric(m.means$roi))
m$roi[m$pos %in% 1:3] <- "Matrix"
m$roi[m$pos %in% 4:8] <- "Compl"
m$roi[m$pos == 9] <- "pre-conj"
m$roi[m$pos == 10] <- "CONJ"
m$roi[m$pos == 11] <- "DET"
m$roi[m$pos == 12] <- "N"
m$roi[m$pos == 13] <- "V"
m$roi[m$pos == 14] <- "SP1"
m$roi[m$pos == 15] <- "SP2"
m$roi[m$pos %in% 16:18] <- "SP3"
# m$roi[!m$pos %in% 3:10] <- "other"
m$roi <- factor(m$roi, levels=c("Matrix", "Compl", "pre-conj", "CONJ", "DET", "N", "V","SP1", "SP2", "SP3"))
# levels(m$roi)
roilabels <- c("Compl", "pre-conj", "CONJ", "DET", "N", "V","SP1", "SP2", "SP3")

##
## DATA CLEAN-UP
##
#load("m.RData")
m.all <- m
m <- subset(m, fail==0)
m <- subset(m, pos!=1 & word!="*")

m$trial <- paste(m$exp, m$iteration, m$cond)
n.trials <- length(unique(m$trial))
m$trialpos <- paste(m$exp, m$iteration, m$cond, m$pos)
#length(unique(m$trialpos))
# head(m)
m$dupl <- duplicated(m$trialpos)
# dim(subset(m,dupl))

## remove trials where si is skipped
#dim(m)
skip9.11 <- subset(m, pos%in%9:11 & skip==1)
skip9.11.trials <- unique(skip9.11$trial)
#length(skip9.11.trials)/n.trials

##
## MEANS
##
n.subj <- length(levels(m$exp))
n.it <- length(levels(m$iteration))
m$FPRT.corr <- 0
library(car)
m$FPRT.corr <- m$FPRT


# TODO: explain
## pos 10 (CONJ) correction for effects of skipping
# boxcox(FPRT ~ skip.prev + skip.next, data=subset(m,pos==10 & FPRT>0 & cond=="UNAMB"))
lm3 <- lm((FPRT)^-.5 ~ skip.prev + skip.next, data=subset(m, pos==10 & FPRT>0 & cond=="UNAMB")); #summary(lm3)
# qqPlot(residuals(lm3))
effect <- predict(lm3) - coef(lm3)[1]
m$FPRT.corr[m$pos==10 & m$FPRT>0 & m$cond=="UNAMB"] <- ((m$FPRT[m$pos==10 & m$FPRT>0 & m$cond=="UNAMB"]^-.5)-effect)^-2

# boxcox(FPRT ~ skip.prev + skip.next , data=subset(m,pos==10 & FPRT>0 & cond!="UNAMB"))
lm4 <- lm(sqrt(FPRT) ~ skip.prev + skip.next, data=subset(m, pos==10 & FPRT>0 & cond!="UNAMB")); #summary(lm4)
# qqPlot(residuals(lm4))
effect <- predict(lm4) - coef(lm4)[1]
m$FPRT.corr[m$pos==10 & m$FPRT>0 & m$cond!="UNAMB"] <- ((m$FPRT[m$pos==10 & m$FPRT>0 & m$cond!="UNAMB"]^.5)-effect)^2

#save(m, file="mv13/m.RData")
@

\begin{figure}[!htbp]
\centering
<<mv13modelambadv,echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, fig.width=5,fig.height=3, out.width='0.8\\textwidth'>>=

## Ambiguity advantage ##
# Reading times ROI, AMB
mlt <- melt(m, id=c("roi","amb","wmc"), measure=c("FFD","FPRT","FPRT.corr","TFT","RPD","retr","enc"), na.rm = T)
cst <- cast(subset(mlt,value>0), variable+roi+amb+wmc ~ ., function(x) c(M=mean(x), SE=sd(x)/sqrt(length(x)), N=length(x), CI=ci(x)))
# head(cst)
means.t3 <- cst
levels(means.t3$variable)[2] <- "Gaze"

#+ AmbAdv, echo=FALSE, fig.keep='all', fig.show='asis', warning=F
## Ambiguity Advantage
means.t3$amb <- as.factor(means.t3$amb)
levels(means.t3$wmc) <- c("high", "low")
colnames(means.t3)[4] <- "W"

(pvdmv13modelrt<-ggplot(aes(amb, M, group=W, lty=W),#, ymax=509, ymin=460), 
data=droplevels(subset(means.t3, (roi%in%c("CONJ") & variable%in%c("FPRT.corr")))))
 + ylab("Mean gaze duration in ms")
 + xlab("")
 + geom_line(position=position_dodge(0.05)) 
 + geom_point(size=3,position=position_dodge(0.05)) 
 + geom_linerange(aes(max=CI.upper, min=CI.lower),position=position_dodge(0.05))
 # + scale_colour_manual("W", values=UPpalette(firstcolor="humfak",secondcolor="darkblue")) + scale_x_discrete(expand=c(0.1,0)) + scale_y_continuous(expand=c(0.0,0))
 # + scale_linetype_manual("W", values = c(1,3))
 # + scale_color_manual("W", values=c("black","gray"))
 + theme_bw()
 + theme(legend.position="right"
  ,legend.key=element_blank()
  ,panel.grid.minor.y=element_blank()
  ,panel.grid.major.y=element_line(color="gray80")
  # ,panel.grid.minor.x=element_line(color="gray60")
  ,panel.grid.minor.x=element_blank()
  ,panel.grid.major.x=element_blank()
  # ,plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), "cm")
  )
)
@
  \caption[Predicted gaze durations by source activation at ambiguous and unambiguous attachments.]{Model predictions for gaze durations at \textit{cuando}/\textit{si} for ambiguous (a and b) and unambiguous (c) conditions, grouped by high and low goal buffer source activation $W$. Error bars represent 95\% confidence intervals.} \label{fig:mv13model:rt}
\end{figure}


\begin{figure}[!htbp]
  \centering
<<mv13modeltimeout,echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, fig.width=5,fig.height=3, out.width='0.8\\textwidth'>>=

## Probabilities
mlt <- melt(m, id=c("roi","cond","wmc"), measure=c("refix","reread","fp_reg","skip","timeout"), na.rm=T)
cst <- cast(mlt, variable+roi+cond+wmc ~ ., function(x) c(M=mean(x), N=length(x)))
cst$SE <- sqrt(cst$M*(1-cst$M))/sqrt(cst$N)
cst$CI.lower <- cst$M + qt(.025, df=cst$N-1) * cst$SE
cst$CI.upper <- cst$M + qt(.975, df=cst$N-1) * cst$SE
means.p <- cst
levels(means.p$variable)[3] <- "reg"
means.p$wmc <- c("high", "low")
colnames(means.p)[4] <- "W"

(ggplot(aes(cond, M, fill=W, group=W), 
  data=droplevels(subset(means.p, variable%in%c("timeout") & roi=="CONJ"))) 
  + ylab("Time-out proportion") 
  + scale_x_discrete(expand=c(0.1,0)) 
  + scale_y_continuous(expand=c(0,0))
  + geom_bar(position=position_dodge(0.9), stat="identity") 
  + geom_linerange(aes(max=CI.upper, min=CI.lower), position=position_dodge(0.9))
  + scale_fill_manual(values=c("gray40","gray70")) 
  + theme_bw() 
  + theme(legend.position="right"
    ,legend.key=element_blank()
    ,panel.grid.minor.y=element_blank()
    ,panel.grid.major.y=element_line(color="gray80")
    ,panel.grid.minor.x=element_blank()
    ,panel.grid.major.x=element_blank()
    )
)

@
  \caption[Predicted time out proportions by source activation at ambiguous and unambiguous attachments.]{Predicted time out proportions for ambiguous high (a), low (b), and unambiguous (c) conditions, grouped by high and low goal buffer source activation $W$. Error bars represent 95\% confidence intervals.} \label{fig:mv13model:timeout}
\end{figure}

Simulated participants were grouped into high and low capacity with respect to the randomly assigned goal source activation $W$ by using a median split (the median of $W$ was \Sexpr{round(median_ga,2)}).
This resulted in $30$ simulated high-capacity subjects with mean \Sexpr{round(ga_means[1],digits=2)} and $30$ simulated low-capacity subjects with mean \Sexpr{round(ga_means[2],digits=2)}.
As in the preceding simulation, no parameter estimation was done to fit the model to the empirical data. 

Model predictions for gaze durations for the potentially ambiguous region \textit{cuando}/\textit{si}\footnote{Predictions are shown for one word only and not for the whole pre-verbal region, because the model currently does not predict any spill-over due to delayed parsing processes. Therefore, the effect of underspecification appears immediately at the critical word. Also see the General Discussion on this point.} 
are plotted in Figure~\ref{fig:mv13model:rt}. The model predicts an ambiguity advantage which is more pronounced for low-$W$ simulations than for high-$W$ simulations. This is in line with the findings of an ambiguity advantage in previous work. 

The timing difference between low- and high-capacity readers in the unambiguous condition predicted by the model is due to different time out proportions in the attachment region, as shown in Figure~\ref{fig:mv13model:timeout}. Because attachment is generally slower for low-capacity readers, more time outs (interface I), which make the eyes wait for the completion of integration, are necessary. No time outs are predicted in the ambiguous conditions, since attachment happens as a process of minor importance in this case.

\begin{figure}[!htbp]
  \centering
<<mv13modelrer,echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, fig.width=6,fig.height=4, out.width='0.7\\textwidth'>>=

## Trial Probabilities
mlt <- melt(trialinfo, id=c("cond","wmc"), measure=c("s.reread","revision","attached","fail","acc","acc2"), na.rm=T)
cst <- cast(mlt, variable+cond+wmc ~ ., function(x) c(M=mean(x), N=length(x)))
cst$SE <- sqrt(cst$M*(1-cst$M))/sqrt(cst$N)
cst$CI.lower <- cst$M + qt(.025, df=cst$N-1) * cst$SE
cst$CI.upper <- cst$M + qt(.975, df=cst$N-1) * cst$SE
# head(cst)
means.p2 <- cst
means.p2$wmc <- c("high", "low")
colnames(means.p2)[3] <- "W"


(ggplot(aes(cond, M, fill=W, group=W), data=droplevels(subset(means.p2, (variable%in%c("s.reread"))))) 
  + ylab("Rereading proportion") 
  + xlab("")
  + scale_fill_manual(values=c("gray40","gray70")) 
  + scale_x_discrete(expand=c(0.1,0)) 
  + scale_y_continuous(expand=c(0,0), limits=c(0,.31))
  + geom_bar(position=position_dodge(0.9), stat="identity") 
  + geom_linerange(aes(max=CI.upper, min=CI.lower), position=position_dodge(0.9))
 + theme_bw()
 + theme(legend.position="right"
  ,legend.key=element_blank()
  ,panel.grid.minor.y=element_blank()
  ,panel.grid.major.y=element_line(color="gray80")
  # ,panel.grid.minor.x=element_line(color="gray60")
  ,panel.grid.minor.x=element_blank()
  ,panel.grid.major.x=element_blank()
  # ,plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), "cm")
  )
 )

@
\caption[Predicted proportions of sentence rereading by source activation at ambiguous and unambiguous attachments.]{Predicted proportions of sentence rereading for ambiguous high (a), low (b), and unambiguous (c) conditions, grouped by high and low goal buffer source activation $W$. Error bars represent 95\% confidence intervals.}
  \label{fig:mv13model:rer}
\end{figure}

\begin{figure}[!htbp]
\centering
<<mv13modelatt, include=TRUE, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, fig.width=6,fig.height=4, out.width='0.7\\textwidth'>>=

## Probabilities
mlt <- melt(m, id=c("roi","cond","wmc"), measure=c("refix","reread","fp_reg","skip","timeout"), na.rm=T)
cst <- cast(mlt, variable+roi+cond+wmc ~ ., function(x) c(M=mean(x), N=length(x)))
cst$SE <- sqrt(cst$M*(1-cst$M))/sqrt(cst$N)
cst$CI.lower <- cst$M + qt(.025, df=cst$N-1) * cst$SE
cst$CI.upper <- cst$M + qt(.975, df=cst$N-1) * cst$SE
means.p <- cst
levels(means.p$variable)[3] <- "reg"
means.p$wmc <- c("high", "low")
colnames(means.p)[4] <- "W"

(ggplot(aes(cond, M, fill=W, group=W), 
  data=droplevels(subset(means.p2, variable%in%c("attached")))) 
  + ylab("Proportion of completed attachments") 
  + scale_x_discrete(expand=c(0.1,0)) 
  + scale_y_continuous(expand=c(0,0))
  + geom_bar(position=position_dodge(0.9), stat="identity") 
  + geom_linerange(aes(max=CI.upper, min=CI.lower), position=position_dodge(0.9))
  + scale_fill_manual(values=c("gray40","gray70")) 
  + theme_bw() 
  + theme(legend.position="right"
    ,legend.key=element_blank()
    ,panel.grid.minor.y=element_blank()
    ,panel.grid.major.y=element_line(color="gray80")
    ,panel.grid.minor.x=element_blank()
    ,panel.grid.major.x=element_blank()
    )
)

@
\caption[Predicted attachment proportions by source activation at ambiguous and unambiguous attachments.]{Predicted attachment proportions for ambiguous high (a), low (b), and unambiguous (c) conditions, grouped by high and low goal buffer source activation $W$. Error bars represent 95\% confidence intervals.}
  \label{fig:mv13model:att}
\end{figure}

Figure~\ref{fig:mv13model:rer} shows the proportions of rereading the sentence after seeing the disambiguating verb \textit{entraron}/\textit{entraran} in the ambiguous conditions or \textit{entraban} in the unambiguous condition. The model predicts rereading almost exclusively in the ambiguous high-attachment condition, because the parser mostly attaches low and reanalysis is therefore mainly needed when the sentence is disambiguated towards high attachment. A second prediction is that high-capacity readers reread in that condition more often than low-capacity readers. 
The reason for this is that simulated high-capacity subjects attach more often than low-capacity subjects, as is shown in Figure~\ref{fig:mv13model:att}. This is the case in both ambiguous conditions, and is responsible for the ambiguity advantage seen for both groups. Rereading is, however, only affected in the ambiguous HIGH-disambiguation condition, since only in this case is reanalysis necessary. 

There are important differences between the observed patterns in the data and the model predictions. This becomes when we compare the predictions in Figure~\ref{fig:mv13model:rer} with the data in Figure~\ref{fig:mv13data:rer}. More reading occurs in high-capacity readers in the ambiguous HIGH condition, and this is consistent with the data. However, the predictions show that rereading proportions of low-capacity readers are also affected by the conditions, which is not the case in the data. Another difference is that, in the data, there is some rereading in every condition; by contrast, the model predictions show rereading only in the reanalysis condition (a). Both differences are due to the simplified nature of the model, which will be elaborated in the discussion.


\subsection{Discussion}
Our simple model of good-enough parsing predicts some of the essential observations: (i) An ambiguity advantage appears at the point of attachment, (ii) low-capacity readers leave an attachment underspecified more often than high-capacity readers, and (iii) high-capacity readers consequently have to reanalyze more often at the point of disambiguation.
Interestingly, these predictions are not the result of different strategies for different groups of working memory capacity; rather, these different behaviors emerge from one common strategy that is an adaptive trade-off between attachment accuracy and reading speed, caused simply by the timing of low-level word identification and oculomotor processes. This simple mechanism leads to an adaptive interaction between parsing, eye movement control, and working memory capacity that predicts the observations mentioned above.

The model used for the above simulation was deliberately kept simple, because the aim was to transparently track predictions to underlying mechanisms. This simplification is responsible for some differences between the model predictions and the empirical observations of \cite{MalsburgVasishth2013}. In the following, four simplifying assumptions in the current model and their implications will be discussed.

Firstly, the model deterministically preferred low attachment (except a small amount of erroneous retrieval of the high attachment site). This explains why rereading is only predicted in the ambiguous HIGH condition (a). In a more realistic model, a utility value for the attachment productions would simulate a non-deterministic preference that would lead to rereading in both ambiguous conditions. However, there is certainly some amount of rereading that is due to other factors than disambiguation; examples are misreadings, erroneous attachments, and other difficulties due to the length and complexity of the sentence. This is clear from the fact that the data also shows rereading in the unambiguous conditions where no reanalysis is necessary per se.

Secondly, the model deterministically completed the attachment in one hundred percent of the simulation runs in the unambiguous condition. In this condition, the model predicted low-capacity readers to be slower than high-capacity readers, because attachment takes longer for the former. For simplicity, underspecification was only allowed in the model for the specific relation of adjunct attachment.
In a more sophisticated model, the trade-off between time out (the eyes wait for the parser) and attachment underspecification (the eyes cut off the parser) would be defined by a utility value which is learned for different attachment relations by reading experience. Thus, also unambiguous and obligatory relations would have some possibility for being underspecified. 
A possible model would be the following:
For every relation, there is a utility value that decides between the two possibilities, time-out (completing the attachment) and cut-off (underspecification). These values are adjusted after every sentence by a positive or negative \emph{reward} according to a task such as answering comprehension questions. An incorrect response to the task will shift the utility towards more accuracy making the completion of attachments more important. A high number of time outs during the sentence reading will, however, shift the utility towards speed, reducing the importance of attachments, leading to more underspecified relations. 
The utility learning module of ACT-R would be suitable for this kind of model. In ACT-R, when a reward is triggered, the utility values $U$ of all productions that fired since the last reward are updated with emphasis on more recent productions as defined in Equation~(\ref{eq:utility}):

\begin{equation}
\label{eq:utility}
U_i(n) = U_i(n-1) + \alpha[R_i(n)-U_i(n-1)]
\end{equation}
where $\alpha$ is the learning rate set by a parameter, $R_i(n)$ is the reward value given to production $i$ at time $n$.

This mechanism would ensure a balance between speed and accuracy according to individual differences and task demands.
For low capacity readers, the utility for attachment would automatically turn out generally lower and thus compensate for slower memory processes. As a result, reading speed of low-capacity readers would be more or less the same as of high-capacity readers. This adaptive, yet simple mechanism of global good-enough processing make the prediction that low-capacity readers do not generally read slower than high-capacity readers; this prediction should be tested in future work.
In addition, a model like this would predict the results of \cite{SwetsDesmetClifton2008} who found indications for underspecification only in sessions when superficial comprehension questions were asked. The adaptation of utility values according to response accuracy would lead to more attachments for comprehension questions targeting the ambiguous relation and more underspecification for easy questions. This adaptation would occur over time and would predict trial effects.

A third simplifying model assumption regards the generation of differences in working memory capacity. Although the data shows qualitative differences in the ambiguity advantage (there is no indication of an ambiguity advantage for high-capacity readers) and rereading proportions (there is no indication of a difference by condition for low-capacity readers), the model rather predicts differences in the magnitude of the ambiguity advantage by capacity level: The ambiguity advantage is predicted to have a larger magnitude for low-capacity readers, and differences in rereading proportions are more pronounced for high-capacity readers. This is a result of the choice made during modelling regarding the variability in capacity between subjects (for the simulations, $W$ is sampled from a normal distribution around $1$ with standard deviation $0.25$). Choosing a greater variance would increase the differences between capacity groups. The variance for $W$ could  be based on the empirical variance in scores of the working memory test that assesses individual working memory capacity. We leave this for future work.

The fourth (and final) point relates to the  immediacy of the predicted effects. In other words, effects are generally predicted at the word that causes them. What is needed is a mechanism that allows for somewhat delayed parsing processes and predicts spill-over as observed, e.g., in the expectation-based first-pass regressions in \cite{Staub2010a} or the ambiguity advantage in gaze durations across the pre-verbal region in \cite{MalsburgVasishth2013}. 

In summary, the model assumes that a simple common mechanism leads to an interaction between parsing, eye movements, and individual differences predicting observations that are attributed to good-enough processing. Further, a more sophisticated model is sketched out that uses adaptive utilities that predict an interaction of underspecifications with task demands.


\section{General Discussion}
In this chapter, three additional interfaces between parsing and eye movement control have been defined and tested. Including the Time Out interface introduced earlier, the framework now comprises three interfaces, which are summarized below:

\begin{enumerate}
\item Time Out: Short regressions compensate for slow syntactic integration, simulation using the Potsdam Sentence Corpus
\item Reanalysis: Immediate attention shift to previous material when structure has to be revised (the simulation of the \cite{Staub2010a} data)
\item Underspecification: \index{Underspecification} For structural relations with low utility, time-consuming attachments are aborted as soon as the next word is ready for integration, such that reading proceeds uninterrupted (the simulation of the \cite{MalsburgVasishth2013} data)
\end{enumerate}

The first two interfaces, Time Out and Reanalysis, cause parser-triggered \emph{interruptions} of the reading process, whereas Underspecification provides mechanisms that abort or delay parsing to ensure \emph{uninterrupted} reading.
Time Out is a relatively straight-forward implementation of a well-defined mechanism. However, the other three interfaces are simplifying descriptions of more complex mechanisms. The \index{Reanalysis interface} Reanalysis interface leaves open the question of where regressions are targeted and how the target position is determined. 
Interface III predicts \emph{when} attachment is aborted due to an interaction of parsing difficulty, word identification timing, and individual differences, but it lacks a definition of \emph{which} relations are eligible candidates for potential underspecification. As discussed above, construal theory provides an orientation for this question, but a continuous experience- and context-based utility value for syntactic relations would be a more realistic model. 

The idea of capacity-constrained sentence comprehension is not new; e.g., the CC READER model \citep{just2002haw,varma2016caps} also has a notion of constrained capacity inducing individual differences in sentence comprehensive behaviour. This model  is currently not in active development or use, but understanding the predictions of the CC READER model for underspecification data would be a very informative  research direction. A completely different way in which capacity could be seen as constrained is by assuming that readers  have different amounts of  parallelism in the number of states that the parser can maintain at any choice point \citep{Jurafsky1996,BostonHaleVasishth2011}. E.g., if we assume that individuals have different amounts of bandwidth for maintaining states in parallel, this could potentially explain individual-level differences. These are all interesting future directions that could be pursued in order to evaluate alternative ways to realize individual-level differences in behaviour in underspecification configurations.
