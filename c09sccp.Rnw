\chapter{Future directions} \label{c09}

In this closing chapter, we briefly discuss what we see as some of the important open problems that we made some progress in solving, and the problems that still need to be addressed. In Section 1.5, we discussed four gaps in the literature. In this book, to what extent did we succeed in addressing these gaps?

\section{Developing implemented computational models}

We have presented several models covering a broad range of empirical phenomena: Subject-verb dependencies (agreement and non-agreement), negative polarity items,  antecedent-reflexive and -reciprocal dependencies, attachment ambiguities, the effect of prominence and cue-associativity,  expectation, reanalysis, and underspecification effects, and relative clause processing. The models were unfortunately not all developed in exactly the same framework; sometimes we implemented full models, at other times simplified models. This mixture of modeling types is a consequence of the fact that the models were developed over many years in different research contexts. Future work should attempt to model the phenomena under a unified framework. It is possible that the python-based implementation of \cite{bdactrbook} is the most appropriate for future work (rather than Common Lisp, the language that ACT-R was originally written in). It would also be very useful if competing models of the phenomena we discuss are also implemented, so that formal model comparison can be carried out. 

\section{An excessive focus on average behaviour}

As discussed earlier, many researchers have pointed out the importance of modelling not just mean differences, but also individual-level differences. Our first attempts in this direction have focussed on individuals with aphasia; this was because this group analyses may make less sense in this population. We also showed how capacity differences between participants can be modelled within the framework presented. However, much more can be done with the ACT-R framework when it comes to understanding the underlying causes of individual-level variation. This line of work has only just begun in our lab \citep{YadavEtAlAMLaP2020}.

\section{Creating higher-precision benchmark data-sets for model evaluation and comparison}

Several classes of problems have been studied in sentence processing. A probably incomplete list is:

\begin{enumerate}
\item Local attachment ambiguities (garden-path constructions)
\item Constraints on dependency completion
\item Shallow vs.\ deep processing (good-enough processing and underspecification)
\item Serial vs.\ parallel parsing
\item The use of syntactic vs.\ semantic (pragmatic/phonological/prosodic) cues in sentence comprehension
\item Illusions of grammaticality/ungrammaticality
\item Similarity-based interference effects
\item Expectation-based effects
\item Large-scale comprehension data on processing differences between unimpaired and impaired populations.
\end{enumerate}

Unfortuntely, psycholinguistics generally neglected power considerations in most of the published literature; as a consequence of the generally low power of published studies, many papers end up reporting over-estimates which never replicate. There is an urgent need to revisit the empirical base of psycholinguistics, and to establish larger-sample benchmark data so that model evaluation and model comparison is meaningful. 

In recent empirical work we have carried out, we have made some modest attempts to systematically create benchmark reading data available specifically on similarity-based interference effects. Examples are 
Mertzen (PhD dissertation), 
\cite{JaegerMertzenVanDykeVasishth2019},
and \cite{VasishthMertzenJaegerGelman2018}. We  have also  tried to systemtically synthesize existing evidence using meta-analysis \citep{JaegerEngelmannVasishth2017}.

An important task that remains for the field is to  systematically create a repository of such data-sets for the different phenomenaa listed above, with the goal that the predictions of competing models of sentence processing be tested against as broad a class of phenomena as possible.

\section{Developing better criteria for evaluating model fit}

The \cite{rp} criteria for what constitutes a good fit has generally been ignored in psycholinguistics. An important goal, which we only partly accomplished in this book, is to constrain model predictions  so that the model evaluation against data is meaningful. An important part such evaluations is to take a model comparison approach.

In our work, \citep[e.g.,][]{NicenboimRetrieval2018,LissonEtAl2020}, we have made some initial attempts to quantitatively compare the predictions of the Lewis and Vasishth activation-based model and McElree's direct-access model against benchmark data. However, a broader investigation that takes a model comparison perspective is yet to be carried out.  

There is also a need to compare the predictive performance of the activation model with other competing modelling  approaches, such as SOPARSE \citep{SmithFranckTaborCogSci2018}. In order to conduct a fair model comparison across completely different approaches such as these, a principled approach needs to be developed for defining which lexical features are relevant for retrieval; currently, this decision is made on an ad-hoc basis. \cite{smith2019smithvasishthfeatures} have taken the first steps towards such a principled approach using large-scale corpora. Such a principled specification of lexical features will be very useful for future attempts at comparing competing models against benchmark data.

\section{In closing}

This book attempts to address many of the important open problems in sentence comprehension, admittedly with varying  degrees of success and varying degrees of uniformity. Nevertheless, the last 20 years of work on retrieval models, starting with the pioneering work of Rick Lewis, have taught us a lot about the role that general information processing principles play in explaining aspects of sentence comprehension. We hope that other researchers will find useful some of the ideas presented in this book. We hope that our first steps, reported in this book, will help others develop the next generation of computational models of sentence processing.

